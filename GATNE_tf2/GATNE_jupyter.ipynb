{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:18.380997Z",
     "start_time": "2019-12-18T10:18:18.376848Z"
    }
   },
   "outputs": [],
   "source": [
    "##Representation Learning for Attributed Multiplex Heterogeneous Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:19.342285Z",
     "start_time": "2019-12-18T10:18:18.384830Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pdb\n",
    "import tqdm\n",
    "from walk import RWGraph\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "from gensim.models.keyedvectors import Vocab\n",
    "from numpy import random\n",
    "from six import iteritems\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import (auc, f1_score, precision_recall_curve, roc_auc_score)\n",
    "from scipy.stats import truncnorm\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:19.347601Z",
     "start_time": "2019-12-18T10:18:19.344358Z"
    }
   },
   "outputs": [],
   "source": [
    "#路径设定\n",
    "file_name = '../Tdata'\n",
    "#超参数设定\n",
    "embedding_size = 200\n",
    "embedding_u_size = 30\n",
    "dim_a = 20 #attention维度\n",
    "att_head = 1\n",
    "neighbor_samples = 10\n",
    "schema = None # metapath的制定schema,此处没有指定\n",
    "num_walks = 20 #路径数\n",
    "walk_length = 10 #\n",
    "window_size = 8 #上下文窗口大小,应小于walk_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:19.358482Z",
     "start_time": "2019-12-18T10:18:19.349404Z"
    }
   },
   "outputs": [],
   "source": [
    "#数据读取\n",
    "def load_training_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    edge_data_by_type = dict()\n",
    "    all_edges = list()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            if words[0] not in edge_data_by_type:\n",
    "                edge_data_by_type[words[0]] = list()\n",
    "            x, y = words[1], words[2]\n",
    "            edge_data_by_type[words[0]].append((x, y))\n",
    "            all_edges.append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    all_edges = list(set(all_edges))\n",
    "    edge_data_by_type['Base'] = all_edges\n",
    "    print('Total training nodes: ' + str(len(all_nodes)))\n",
    "    return edge_data_by_type\n",
    "def load_testing_data(f_name):\n",
    "    print('We are loading data from:', f_name)\n",
    "    true_edge_data_by_type = dict()\n",
    "    false_edge_data_by_type = dict()\n",
    "    all_edges = list()\n",
    "    all_nodes = list()\n",
    "    with open(f_name, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line[:-1].split(' ')\n",
    "            x, y = words[1], words[2]\n",
    "            if int(words[3]) == 1:\n",
    "                if words[0] not in true_edge_data_by_type:\n",
    "                    true_edge_data_by_type[words[0]] = list()\n",
    "                true_edge_data_by_type[words[0]].append((x, y))\n",
    "            else:\n",
    "                if words[0] not in false_edge_data_by_type:\n",
    "                    false_edge_data_by_type[words[0]] = list()\n",
    "                false_edge_data_by_type[words[0]].append((x, y))\n",
    "            all_nodes.append(x)\n",
    "            all_nodes.append(y)\n",
    "    all_nodes = list(set(all_nodes))\n",
    "    return true_edge_data_by_type, false_edge_data_by_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:19.369646Z",
     "start_time": "2019-12-18T10:18:19.359918Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are loading data from: ../Tdata/train.txt\n",
      "Total training nodes: 511\n",
      "We are loading data from: ../Tdata/valid.txt\n",
      "We are loading data from: ../Tdata/test.txt\n"
     ]
    }
   ],
   "source": [
    "network_data = load_training_data(file_name + '/train.txt')\n",
    "valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(file_name + '/valid.txt')\n",
    "testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(file_name + '/test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:19.377650Z",
     "start_time": "2019-12-18T10:18:19.370806Z"
    }
   },
   "outputs": [],
   "source": [
    "#路径生成\n",
    "#从edgse生成Graph\n",
    "def get_G_from_edges(edges):\n",
    "    edge_dict = dict()\n",
    "    for edge in edges:\n",
    "        edge_key = str(edge[0]) + '_' + str(edge[1])\n",
    "        if edge_key not in edge_dict:\n",
    "            edge_dict[edge_key] = 1\n",
    "        else:\n",
    "            edge_dict[edge_key] += 1\n",
    "    tmp_G = nx.Graph()\n",
    "    for edge_key in edge_dict:\n",
    "        weight = edge_dict[edge_key]\n",
    "        x = edge_key.split('_')[0]\n",
    "        y = edge_key.split('_')[1]\n",
    "        tmp_G.add_edge(x, y)\n",
    "        tmp_G[x][y]['weight'] = weight\n",
    "    return tmp_G\n",
    "\n",
    "def generate_walks(network_data):\n",
    "    base_network = network_data['Base']\n",
    "    #if schema is not None:\n",
    "    #    node_type = load_node_type('../Tdata/node_type.txt')\n",
    "    #else:\n",
    "    #    node_type = None\n",
    "    node_type = None\n",
    "        \n",
    "    base_walker = RWGraph(get_G_from_edges(base_network), node_type=node_type)\n",
    "    base_walks = base_walker.simulate_walks(num_walks, walk_length, schema=None)\n",
    "    all_walks = []\n",
    "    for layer_id in network_data:\n",
    "        if layer_id == 'Base':\n",
    "            continue\n",
    "\n",
    "        tmp_data = network_data[layer_id]\n",
    "        # start to do the random walk on a layer\n",
    "\n",
    "        layer_walker = RWGraph(get_G_from_edges(tmp_data))\n",
    "        layer_walks = layer_walker.simulate_walks(num_walks, walk_length)\n",
    "\n",
    "        all_walks.append(layer_walks)\n",
    "\n",
    "    print('路径生成完毕！')\n",
    "\n",
    "    return base_walks, all_walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.326586Z",
     "start_time": "2019-12-18T10:18:19.378933Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "路径生成完毕！\n"
     ]
    }
   ],
   "source": [
    "base_walks, all_walks = generate_walks(network_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.335743Z",
     "start_time": "2019-12-18T10:18:21.329251Z"
    }
   },
   "outputs": [],
   "source": [
    "#生成index2word\n",
    "def generate_vocab(all_walks):\n",
    "    index2word = []\n",
    "    raw_vocab = defaultdict(int)\n",
    "\n",
    "    for walks in all_walks:\n",
    "        for walk in walks:\n",
    "            for word in walk:\n",
    "                raw_vocab[word] += 1\n",
    "\n",
    "    vocab = {}\n",
    "    for word, v in iteritems(raw_vocab):\n",
    "        vocab[word] = Vocab(count=v, index=len(index2word))\n",
    "        index2word.append(word)\n",
    "\n",
    "    index2word.sort(key=lambda word: vocab[word].count, reverse=True)\n",
    "    for i, word in enumerate(index2word):\n",
    "        vocab[word].index = i\n",
    "    \n",
    "    return vocab, index2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.350064Z",
     "start_time": "2019-12-18T10:18:21.338025Z"
    }
   },
   "outputs": [],
   "source": [
    "vocab, index2word = generate_vocab([base_walks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.394094Z",
     "start_time": "2019-12-18T10:18:21.351548Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature dimension: 142\n"
     ]
    }
   ],
   "source": [
    "feature_dic = {}\n",
    "with open(file_name+'/feature.txt', 'r') as f:\n",
    "    first = True\n",
    "    for line in f:\n",
    "        if first:\n",
    "            first = False\n",
    "            continue\n",
    "        items = line.strip().split()\n",
    "        feature_dic[items[0]] = items[1:]\n",
    "if feature_dic is not None:\n",
    "    feature_dim = len(list(feature_dic.values())[0])\n",
    "    print('feature dimension: ' + str(feature_dim))\n",
    "    features = np.zeros((len(vocab), feature_dim), dtype=np.float32)\n",
    "    for key, value in feature_dic.items():\n",
    "        if key in vocab:\n",
    "            features[vocab[key].index, :] = np.array(value)\n",
    "feature_dic = torch.Tensor(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.401085Z",
     "start_time": "2019-12-18T10:18:21.396157Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pairs(all_walks, vocab):\n",
    "    pairs = []\n",
    "    skip_window = window_size // 2\n",
    "    for layer_id, walks in enumerate(all_walks):\n",
    "        for walk in walks:\n",
    "            for i in range(len(walk)):\n",
    "                for j in range(1, skip_window + 1):\n",
    "                    if i - j >= 0:\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))\n",
    "                    if i + j < len(walk):\n",
    "                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.832209Z",
     "start_time": "2019-12-18T10:18:21.402950Z"
    }
   },
   "outputs": [],
   "source": [
    "train_pairs = generate_pairs(all_walks, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.837373Z",
     "start_time": "2019-12-18T10:18:21.833837Z"
    }
   },
   "outputs": [],
   "source": [
    "# 确保Base边在最后一个\n",
    "edge_types = list(network_data.keys())\n",
    "if edge_types[-1] != 'Base':\n",
    "    edge_types.sort()\n",
    "    edge_types.remove('Base')\n",
    "    edge_types.append('Base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.842637Z",
     "start_time": "2019-12-18T10:18:21.839073Z"
    }
   },
   "outputs": [],
   "source": [
    "# 网络参数计算\n",
    "num_nodes = len(index2word)\n",
    "edge_type_count = len(edge_types) - 1\n",
    "u_num = edge_type_count\n",
    "neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.874639Z",
     "start_time": "2019-12-18T10:18:21.844717Z"
    }
   },
   "outputs": [],
   "source": [
    "#根据neighbor_samples去截断、填充邻居，如果一个实体点没有邻居，则认为它的邻居是他自己\n",
    "for r in range(edge_type_count):\n",
    "    g = network_data[edge_types[r]]\n",
    "    for (x, y) in g:\n",
    "        ix = vocab[x].index\n",
    "        iy = vocab[y].index\n",
    "        neighbors[ix][r].append(iy)\n",
    "        neighbors[iy][r].append(ix)\n",
    "    for i in range(num_nodes):\n",
    "        if len(neighbors[i][r]) == 0:\n",
    "            neighbors[i][r] = [i] * neighbor_samples\n",
    "        elif len(neighbors[i][r]) < neighbor_samples:\n",
    "            neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
    "        elif len(neighbors[i][r]) > neighbor_samples:\n",
    "            neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.881364Z",
     "start_time": "2019-12-18T10:18:21.877137Z"
    }
   },
   "outputs": [],
   "source": [
    "# 进行负采样\n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    batch_size = targets.size(0)\n",
    "    neg_samples = []\n",
    "    for i in range(batch_size):\n",
    "        nsample = []\n",
    "        target_index = targets[i].item()\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            if neg == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        neg_samples.append(np.array(nsample)) \n",
    "    return np.array(neg_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.893927Z",
     "start_time": "2019-12-18T10:18:21.883852Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, true_edges, false_edges):\n",
    "\n",
    "    true_list = list()\n",
    "    prediction_list = list()\n",
    "    true_num = 0\n",
    "    for edge in true_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        \n",
    "        if tmp_score is not None:\n",
    "            true_list.append(1)\n",
    "            prediction_list.append(tmp_score)\n",
    "            true_num += 1\n",
    "    for edge in false_edges:\n",
    "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
    "        \n",
    "        if tmp_score is not None:\n",
    "            true_list.append(0)\n",
    "            prediction_list.append(tmp_score)\n",
    "\n",
    "    sorted_pred = prediction_list[:]\n",
    "    sorted_pred.sort()\n",
    "    threshold = sorted_pred[-true_num]\n",
    "    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n",
    "    for i in range(len(prediction_list)):\n",
    "        if prediction_list[i] >= threshold:\n",
    "            y_pred[i] = 1\n",
    "    y_true = np.array(true_list)\n",
    "    y_scores = np.array(prediction_list)\n",
    "    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n",
    "    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.899837Z",
     "start_time": "2019-12-18T10:18:21.895791Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_score(local_model, node1, node2):\n",
    "\n",
    "    try:\n",
    "        vector1 = np.array(local_model[node1].view(-1).tolist())\n",
    "        vector2 = np.array(local_model[node2].view(-1).tolist())\n",
    "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
    "    except Exception as e:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.907499Z",
     "start_time": "2019-12-18T10:18:21.901696Z"
    }
   },
   "outputs": [],
   "source": [
    "def getBatch(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])\n",
    "        yield (np.array(x).astype(np.int32), np.array(y).reshape(-1, 1).astype(np.int32), np.array(t).astype(np.int32), np.array(neigh).astype(np.int32)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.913718Z",
     "start_time": "2019-12-18T10:18:21.909600Z"
    }
   },
   "outputs": [],
   "source": [
    "def truncated_normal_(tensor, mean=0, std=1):\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.932078Z",
     "start_time": "2019-12-18T10:18:21.916027Z"
    }
   },
   "outputs": [],
   "source": [
    "# 网络结构,核心代码\n",
    "class MyLayer(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, neighbor_samples, feature_dic):\n",
    "        super(MyLayer, self).__init__()\n",
    "        # attention参数传递\n",
    "        self.neighbor_samples = neighbor_samples\n",
    "        self.u_num = u_num\n",
    "        self.att_head = att_head\n",
    "        self.embedding_size = embedding_size\n",
    "        self.feature_dim = feature_dic.shape[1]\n",
    "        self.edge_type_count = edge_type_count\n",
    "        self.embedding_u_size = embedding_u_size\n",
    "        self.feature_dic = feature_dic # Tensor(node_num*feature_dim)\n",
    "        # 初始化权重\n",
    "        self.neigh_feature_trans = nn.Parameter(torch.Tensor(edge_type_count, feature_dim, embedding_u_size))\n",
    "        truncated_normal_(self.neigh_feature_trans,std=1.0 / math.sqrt(embedding_size))\n",
    "        self.neigh_linear_1 = nn.Parameter(torch.Tensor(edge_type_count, embedding_u_size, dim_a))\n",
    "        truncated_normal_(self.neigh_linear_1,std=1.0 / math.sqrt(embedding_size))\n",
    "        self.neigh_linear_2 = nn.Parameter(torch.Tensor(edge_type_count, dim_a, att_head))\n",
    "        truncated_normal_(self.neigh_linear_2,std=1.0 / math.sqrt(embedding_size))\n",
    "        self.neigh_linear_last = nn.Parameter(torch.Tensor(edge_type_count, embedding_u_size, embedding_size // att_head))\n",
    "        truncated_normal_(self.neigh_linear_last,std=1.0 / math.sqrt(embedding_size))\n",
    "        self.feature_layer1 = nn.Parameter(torch.Tensor(feature_dim, embedding_size))\n",
    "        truncated_normal_(self.feature_layer1,std=1.0 / math.sqrt(embedding_size))\n",
    "        self.feature_layer2 = nn.Parameter(torch.Tensor(feature_dim, embedding_size))\n",
    "        truncated_normal_(self.feature_layer2)\n",
    "    def forward(self,input_node,neigh_type,node_neigh):\n",
    "        # input_node:LongTensor(1) 输入实体点\n",
    "        # node_neigh:LongTensor(edge_type_num*neigh_sample) 输入实体点的邻居\n",
    "        # neigh_type:LongTensor(1) 实体之间的关系\n",
    "        batch_size = max(1,input_node.shape[0])\n",
    "        node_feature = torch.index_select(self.feature_dic, 0, input_node)\n",
    "        # pdb.set_trace()# 提取输入node的特征\n",
    "        node_embed = torch.matmul(node_feature,self.feature_layer1)\n",
    "        node_weight = torch.matmul(node_feature,self.feature_layer2)\n",
    "        #pdb.set_trace()\n",
    "        node_neigh = torch.unbind(node_neigh, dim=1)\n",
    "        neigh_type_embedding = torch.empty([batch_size,self.edge_type_count,self.embedding_u_size]).cuda()\n",
    "        for j in range(batch_size):\n",
    "            neigh_feature_temp = torch.cat([torch.matmul(torch.index_select(self.feature_dic, 0, node_neigh[i][j]).reshape(-1, self.feature_dim), self.neigh_feature_trans[i].reshape(self.feature_dim, self.embedding_u_size)) for i in range(self.edge_type_count)],0)           \n",
    "            neigh_feature_temp = neigh_feature_temp.reshape(self.edge_type_count, -1, self.neighbor_samples, self.embedding_u_size)\n",
    "            neigh_type_embedding[j] = torch.mean(neigh_feature_temp,2).transpose(1,0)\n",
    "        # attention层\n",
    "        # pdb.set_trace()\n",
    "        trans_w_s1 = torch.index_select(self.neigh_linear_1, 0, neigh_type)\n",
    "        trans_w_s2 = torch.index_select(self.neigh_linear_2, 0, neigh_type)\n",
    "        trans_w = torch.index_select(self.neigh_linear_last, 0, neigh_type)\n",
    "        attention = nn.Tanh()(torch.matmul(neigh_type_embedding,trans_w_s1))\n",
    "        attention = torch.matmul(attention,trans_w_s2)\n",
    "        attention = nn.Softmax()(attention.view(-1,self.u_num))\n",
    "        attention = attention.view(-1, self.att_head, self.u_num)        \n",
    "        node_type_embed = torch.matmul(attention,neigh_type_embedding)\n",
    "        node_embed = node_embed + torch.matmul(node_type_embed,trans_w).view(-1,self.embedding_size) + node_weight\n",
    "        last_node_embed = nn.functional.normalize(node_embed, p=2, dim=1, eps=1e-12, out=None)\n",
    "        return last_node_embed.reshape(batch_size,1,-1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.944535Z",
     "start_time": "2019-12-18T10:18:21.934169Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self,num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, feature_dic, neighbor_samples):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.Embedding_layer = MyLayer(num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, neighbor_samples,feature_dic)  \n",
    "        self.embedding_u = nn.Embedding(num_nodes, embedding_size)\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_sampled = 5\n",
    "    def forward(self, input_node, node_neigh, neigh_type, label_node):\n",
    "        n = input_node.shape[0]\n",
    "        #pdb.set_trace()\n",
    "        weights = torch.ones((self.num_nodes,), dtype=torch.float) / (self.num_nodes)                                                              \n",
    "        #weights[label_node.tolist()] = 0\n",
    "        negs = torch.multinomial(\n",
    "            weights, self.num_sampled * n, replacement=True\n",
    "        ).view(n, self.num_sampled).cuda()\n",
    "        center_embeds = self.Embedding_layer(input_node,neigh_type,node_neigh)\n",
    "        log_target = torch.log(torch.sigmoid(torch.sum(torch.bmm(self.embedding_u(label_node),center_embeds.transpose(1, 2)), 1))).squeeze()\n",
    "        noise = torch.neg(self.embedding_u(negs))\n",
    "        sum_log_sampled = torch.sum(\n",
    "            torch.log(torch.sigmoid(torch.bmm(noise, center_embeds.transpose(1, 2)))), 1\n",
    "        ).squeeze()\n",
    "        loss = log_target + sum_log_sampled        \n",
    "        return -loss.sum() / n\n",
    "    def prediction(self,input_node,neigh_type,node_neigh):\n",
    "        node_neigh = node_neigh.unsqueeze(0)\n",
    "        return self.Embedding_layer(input_node,neigh_type,node_neigh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:21.950267Z",
     "start_time": "2019-12-18T10:18:21.947476Z"
    }
   },
   "outputs": [],
   "source": [
    "# 模型训练参数\n",
    "BATCH_SIZE =  1000\n",
    "EPOCH = 100\n",
    "NEG = 3\n",
    "set_patience = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T10:18:26.200789Z",
     "start_time": "2019-12-18T10:18:21.951867Z"
    }
   },
   "outputs": [],
   "source": [
    "losses = []\n",
    "model = MyNet(num_nodes, embedding_size, u_num, dim_a, att_head, edge_type_count, feature_dic.cuda(), neighbor_samples).cuda()\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "g_iter = 0\n",
    "best_score = 0\n",
    "patience = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T11:19:37.365912Z",
     "start_time": "2019-12-18T10:18:26.202357Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0, mean_loss : 4.78\n",
      "Epoch : 0, mean_loss : 3.82\n",
      "Epoch : 0, mean_loss : 3.23\n",
      "Epoch : 0, mean_loss : 2.93\n",
      "Epoch : 0, mean_loss : 2.74\n",
      "Epoch : 0, mean_loss : 2.61\n",
      "Epoch : 0, mean_loss : 2.51\n",
      "Epoch : 0, mean_loss : 2.42\n",
      "valid auc: 0.8211900052459129\n",
      "valid pr: 0.7607368396149966\n",
      "valid f1: 0.7249266862170087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1, mean_loss : 1.83\n",
      "Epoch : 1, mean_loss : 1.74\n",
      "Epoch : 1, mean_loss : 1.72\n",
      "Epoch : 1, mean_loss : 1.69\n",
      "Epoch : 1, mean_loss : 1.67\n",
      "Epoch : 1, mean_loss : 1.65\n",
      "Epoch : 1, mean_loss : 1.63\n",
      "Epoch : 1, mean_loss : 1.61\n",
      "valid auc: 0.8622485831735193\n",
      "valid pr: 0.8365777664499239\n",
      "valid f1: 0.7620967741935484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 2, mean_loss : 1.42\n",
      "Epoch : 2, mean_loss : 1.44\n",
      "Epoch : 2, mean_loss : 1.44\n",
      "Epoch : 2, mean_loss : 1.43\n",
      "Epoch : 2, mean_loss : 1.42\n",
      "Epoch : 2, mean_loss : 1.42\n",
      "Epoch : 2, mean_loss : 1.41\n",
      "Epoch : 2, mean_loss : 1.40\n",
      "valid auc: 0.8999795323397632\n",
      "valid pr: 0.8801466924083264\n",
      "valid f1: 0.8172287390029326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 3, mean_loss : 1.36\n",
      "Epoch : 3, mean_loss : 1.34\n",
      "Epoch : 3, mean_loss : 1.33\n",
      "Epoch : 3, mean_loss : 1.33\n",
      "Epoch : 3, mean_loss : 1.32\n",
      "Epoch : 3, mean_loss : 1.32\n",
      "Epoch : 3, mean_loss : 1.32\n",
      "Epoch : 3, mean_loss : 1.31\n",
      "valid auc: 0.9190645290288182\n",
      "valid pr: 0.9018667885730467\n",
      "valid f1: 0.8335043988269795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 4, mean_loss : 1.27\n",
      "Epoch : 4, mean_loss : 1.28\n",
      "Epoch : 4, mean_loss : 1.28\n",
      "Epoch : 4, mean_loss : 1.28\n",
      "Epoch : 4, mean_loss : 1.27\n",
      "Epoch : 4, mean_loss : 1.27\n",
      "Epoch : 4, mean_loss : 1.27\n",
      "Epoch : 4, mean_loss : 1.26\n",
      "valid auc: 0.9244341723927383\n",
      "valid pr: 0.9075706292216663\n",
      "valid f1: 0.8513196480938416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 5, mean_loss : 1.23\n",
      "Epoch : 5, mean_loss : 1.25\n",
      "Epoch : 5, mean_loss : 1.25\n",
      "Epoch : 5, mean_loss : 1.24\n",
      "Epoch : 5, mean_loss : 1.24\n",
      "Epoch : 5, mean_loss : 1.24\n",
      "Epoch : 5, mean_loss : 1.24\n",
      "Epoch : 5, mean_loss : 1.24\n",
      "valid auc: 0.9253007370077656\n",
      "valid pr: 0.9099098618654611\n",
      "valid f1: 0.8399560117302054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 6, mean_loss : 1.21\n",
      "Epoch : 6, mean_loss : 1.22\n",
      "Epoch : 6, mean_loss : 1.22\n",
      "Epoch : 6, mean_loss : 1.22\n",
      "Epoch : 6, mean_loss : 1.22\n",
      "Epoch : 6, mean_loss : 1.22\n",
      "Epoch : 6, mean_loss : 1.22\n",
      "Epoch : 6, mean_loss : 1.22\n",
      "valid auc: 0.9242714415940696\n",
      "valid pr: 0.9092114346515745\n",
      "valid f1: 0.8367302052785925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 7, mean_loss : 1.18\n",
      "Epoch : 7, mean_loss : 1.21\n",
      "Epoch : 7, mean_loss : 1.21\n",
      "Epoch : 7, mean_loss : 1.21\n",
      "Epoch : 7, mean_loss : 1.21\n",
      "Epoch : 7, mean_loss : 1.21\n",
      "Epoch : 7, mean_loss : 1.21\n",
      "Epoch : 7, mean_loss : 1.21\n",
      "valid auc: 0.923646092224869\n",
      "valid pr: 0.90874018565069\n",
      "valid f1: 0.8367302052785925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 8, mean_loss : 1.21\n",
      "Epoch : 8, mean_loss : 1.21\n",
      "Epoch : 8, mean_loss : 1.20\n",
      "Epoch : 8, mean_loss : 1.20\n",
      "Epoch : 8, mean_loss : 1.20\n",
      "Epoch : 8, mean_loss : 1.20\n",
      "Epoch : 8, mean_loss : 1.20\n",
      "Epoch : 8, mean_loss : 1.20\n",
      "valid auc: 0.9230831778192483\n",
      "valid pr: 0.9089563322772238\n",
      "valid f1: 0.8399560117302054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 9, mean_loss : 1.22\n",
      "Epoch : 9, mean_loss : 1.19\n",
      "Epoch : 9, mean_loss : 1.19\n",
      "Epoch : 9, mean_loss : 1.19\n",
      "Epoch : 9, mean_loss : 1.19\n",
      "Epoch : 9, mean_loss : 1.19\n",
      "Epoch : 9, mean_loss : 1.19\n",
      "Epoch : 9, mean_loss : 1.19\n",
      "valid auc: 0.9206793134733964\n",
      "valid pr: 0.9076269419642229\n",
      "valid f1: 0.8431818181818183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/wld_pytorch/lib/python3.7/site-packages/ipykernel_launcher.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 10, mean_loss : 1.22\n",
      "Epoch : 10, mean_loss : 1.19\n",
      "Epoch : 10, mean_loss : 1.18\n",
      "Epoch : 10, mean_loss : 1.18\n",
      "Epoch : 10, mean_loss : 1.18\n",
      "Epoch : 10, mean_loss : 1.18\n",
      "Epoch : 10, mean_loss : 1.18\n",
      "Epoch : 10, mean_loss : 1.18\n",
      "valid auc: 0.9234407383837429\n",
      "valid pr: 0.9089009428858608\n",
      "valid f1: 0.8431818181818183\n",
      "Early Stopping\n",
      "Overall ROC-AUC: 0.9336145414474002\n",
      "Overall PR-AUC 0.9088397518884685\n",
      "Overall F1: 0.8619035458923099\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCH):\n",
    "    random.shuffle(train_pairs)\n",
    "    batches = getBatch(train_pairs,neighbors,BATCH_SIZE)\n",
    "    avg_loss = 0.0\n",
    "    for i,batch in enumerate(getBatch(train_pairs,neighbors,BATCH_SIZE)):\n",
    "\n",
    "        input_node = torch.LongTensor(batch[0]).cuda()\n",
    "        label_node = torch.LongTensor(batch[1]).cuda()\n",
    "        neigh_type = torch.LongTensor(batch[2]).cuda()\n",
    "        node_neigh = torch.LongTensor(batch[3]).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(input_node, node_neigh, neigh_type, label_node)        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.data.tolist()\n",
    "        if i % 100 == 0:\n",
    "            print(\"Epoch : %d, mean_loss : %.02f\" % (epoch, avg_loss / (i + 1)))\n",
    "        #if i>2:break\n",
    "\n",
    "    final_model = dict(zip(edge_types[:-1], [dict() for _ in range(edge_type_count)]))\n",
    "    for i in range(edge_type_count):\n",
    "        for j in range(num_nodes):\n",
    "            input_node = torch.LongTensor([j]).cuda()\n",
    "            node_neigh = torch.LongTensor(neighbors[j]).cuda()\n",
    "            neigh_type = torch.LongTensor([i]).cuda()\n",
    "            # pdb.set_trace()\n",
    "            final_model[edge_types[i]][index2word[j]] = model.prediction(input_node,neigh_type,node_neigh)\n",
    "            \n",
    "    valid_aucs, valid_f1s, valid_prs = [], [], []\n",
    "    test_aucs, test_f1s, test_prs = [], [], []\n",
    "    for i in range(edge_type_count):\n",
    "        # pdb.set_trace()\n",
    "        tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], valid_true_data_by_edge[edge_types[i]], valid_false_data_by_edge[edge_types[i]])\n",
    "        valid_aucs.append(tmp_auc)\n",
    "        valid_f1s.append(tmp_f1)\n",
    "        valid_prs.append(tmp_pr)\n",
    "\n",
    "        tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], testing_true_data_by_edge[edge_types[i]], testing_false_data_by_edge[edge_types[i]])\n",
    "        test_aucs.append(tmp_auc)\n",
    "        test_f1s.append(tmp_f1)\n",
    "        test_prs.append(tmp_pr)\n",
    "    print('valid auc:', np.mean(valid_aucs))\n",
    "    print('valid pr:', np.mean(valid_prs))\n",
    "    print('valid f1:', np.mean(valid_f1s))\n",
    "\n",
    "    average_auc = np.mean(test_aucs)\n",
    "    average_f1 = np.mean(test_f1s)\n",
    "    average_pr = np.mean(test_prs)\n",
    "\n",
    "    cur_score = np.mean(valid_aucs)\n",
    "    if cur_score > best_score:\n",
    "        best_score = cur_score\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience > set_patience:\n",
    "            print('Early Stopping')\n",
    "            print('Overall ROC-AUC:',str(average_auc))\n",
    "            print('Overall PR-AUC',str(average_pr) )\n",
    "            print('Overall F1:',str(average_f1))\n",
    "            output_name = '../Tdata/result_pytorch.txt'\n",
    "            f = open(output_name,'w+')\n",
    "            f.write('Overall ROC-AUC:' + str(average_auc) + '\\n')\n",
    "            f.write('Overall PR-AUC'+ str(average_pr) + '\\n')\n",
    "            f.write('Overall F1:'+ str(average_f1) + '\\n')   \n",
    "            break   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name=input()\n",
    "print('hello,',name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "“wld_pytorch”",
   "language": "python",
   "name": "wld_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
